---
title: "Parallel computation in R"
author:
  - vgranda
  - emf
date: "2023-09-04"
date_lastmod: "2023-09-04"
format:
  hugo-md:
    mermaid-format: svg
    keep-yaml: false
    preserve-yaml: false
knitr:
  opts_chunk: 
    collapse: true
editor_options: 
  chunk_output_type: console
---

## Introduction

This document will explain the basic concepts of parallel computing in R, with code examples to
illustrate the concepts presented here. The topics covered include:

-   What is parallel computing?\
-   When can we use it?\
-   A little introduction to loops and maps in R (`for`, `lapply`, `map`...)\
-   Ways to use parallelization in your code (`parallel`, `furrr`...)\
-   How to check if is worth the hassle\

## What is parallel computing?

```{dot}
//| label: fig-cpu
//| fig-cap: "Modern CPU and cores"
digraph G {
  edge [style=invis];
  subgraph cluster_cpu1 {
    node [shape=Msquare,style=filled,color=white];
    style=filled;
    color=lightgrey;
    core2; core1; core4; core3;
    label = "CPU #1";
  }
  subgraph cluster_cpu2 {
    node [shape=Msquare,style=filled,color=white];
    style=filled;
    color=lightgrey;
    core6; core5; core8; core7;
    label = "CPU #2";
  }
  core1 -> core3; core2 -> core4;
  core5 -> core7; core6 -> core8;
}
```

First of all we need to understand a little about CPUs (Central Processing Unit) and cores.
Modern computers (@fig-cpu) have multiple CPUs, and those can have one or multiple cores. Each core
is responsible of running individual processes.  

Think of a simple algebraic operation, adding two numbers (`1 + 1`). In a nutshell, that operation
is translated to machine code and a process is started in one core to perform the operation
(@fig-sum).

```{dot}
//| label: fig-sum
//| fig-cap: "One core is performing the '1 +1' operation. This leaves the other cores available to concurrently start other procesess"
digraph G {
  edge [style=invis];
  subgraph cluster_cpu1 {
    node [shape=Msquare,style=filled,color=green, label="Available\n core"];
    style=filled;
    color=lightgrey;
    core2; core1; core4; core3 [shape=Msquare,style=filled,color=red, label="'1 + 1'\nprocess running"];
    label = "CPU #1";
  }
  subgraph cluster_cpu2 {
    node [shape=Msquare,style=filled,color=green, label="Available\n core"];
    style=filled;
    color=lightgrey;
    core6; core5; core8; core7;
    label = "CPU #2";
  }
  core1 -> core3; core2 -> core4;
  core5 -> core7; core6 -> core8;
}
```

So CPU cores can be used to run the the same process with different data in parallel to speed up
long tasks. **In theory**, this can make things $1/n_{cores}$ faster, but in reality, other factors
must be added (time consumed transferring data to each process, time consumed gathering results
from different processes and join them, time consumed spawning new processes...) so the time
gain highly depends on the type of operations, data types used...

::: {.callout-note}
In fact, sometimes workflows are slower when parallelized, so we always need to check if we are really
saving time and effort. See [below](#hassle) for more information on this.
:::

#### R is a single process software

`R` is designed to run in only one CPU process. This is due to the time when `S` (`R` predecessor)
and `R` were developed, where CPUs had mostly one core and multitasking and parallel computation
were still not widely available or technologies were still undeveloped.

Given the `R` limitations explained before, parallel computing in `R` is not available
*out-of-the-box*. We will need to use extra packages/libraries and we can only use it in
specific cases and data types.

## When can we use it?

You have been using parallel computing in `R` without knowing it. A lot of `R` base functions are
calls to methods written in languages that support multitasking (`C++`, `Rust`...). For example,
matrix operations and other linear algebra functions (common operations when calculating
regression coefficients when using `lm` and other model functions) are calls to `C++` methods that
are parallelized and use the CPU cores available in your system (@lst-matrix)

```{r}
#| lst-label: lst-matrix
#| lst-cap: "Time consumed by matrix operations. We can see that the user time is bigger than the elapsed time. This means that the task (matrix product) was parallelized consuming more CPU time (user), but less real time (elapsed)"
observations <- matrix(rnorm(1e6 * 100), 1e6, 100)
predictions <- rnorm(100)
system.time(outcome <- drop(observations %*% predictions) + rnorm(1e6))
```

::: {.callout-note}
Some other `R` packages have implemented the methods we'll explain in the following sections and
offer arguments to the user to choose if and how parallelization must be done. For example, `boot`
package offer parallelization options when bootstrapping model coefficients.
:::

### Working with embarrassingly parallel problems

*[Embarrassingly parallel problems](https://en.wikipedia.org/wiki/Embarrassingly_parallel)*
are those where we can easily separate the problem into several parallel tasks
^[Also known as *perfectly parallel*, *delightfully parallel* or *pleasingly parallel* problems,
but those names don't have that ring on it]. This kind of problems are very usual
in scientific and statistics fields. Think of the classic data analysis showed below
(@fig-daworkflow).

```{mermaid}
%%| label: fig-daworkflow
%%| fig-cap: "Classic data analysis workflow"
%%{init: {"flowchart": {"htmlLabels": false, "defaultRenderer": "dagre"}} }%%
flowchart LR
  subgraph single[Process 1]
    direction LR
    read[Read file] --> process[Transform data] --> model[Model data] --> visualize[Visualize results]
  end
  single -->|"`Repeat after
  we finish
  with a file`"| single
```

In this process, we need to ingest the data, processing it to clean/transform/... it, modelling
the data and finally visualize/store the results. Now imagine we have to repeat the same process for
hundred or thousands of data files (*i.e.*, remote sensing images, genomic analyses, historical and 
projections climatic analyses...). Instead of processing each task one after another (in a
sequential way) we can divide the input (names of the files to read) in chunks and send each chunk
to CPU processes that run in parallel, which can save a lot of time and effort (@fig-daworflowpar).

```{mermaid}
%%| label: fig-daworflowpar
%%| fig-cap: "Same data analysis workflow as before but running in parallel, each process in a different CPU core"
%%{init: {"flowchart": {"htmlLabels": false, "defaultRenderer": "dagre"}} }%%
flowchart LR
  subgraph one[Process 1]
    direction LR
    read[Read file] --> process[Transform data] --> model[Model data] --> visualize[Visualize results]
  end
  subgraph two[Process 2]
    direction LR
    read_2[Read file] --> process_2[Transform data] --> model_2[Model data] --> visualize_2[Visualize results]
  end
  subgraph three[Process 3]
    direction LR
    read_n[Read file] --> process_n[Transform data] --> model_n[Model data] --> visualize_n[Visualize results]
  end
  file_name[File names] --> one & two & three
  one --> one_finish[done]
  two --> two_finish[done]
  three --> three_finish[done]
```


This kind of *embarrasingly parallel tasks* are the ones that beneficiate most of parallelization.

## A little introduction to loops and maps in R (`for`, `lapply`, `map`...)

### Loops

We talk before about *embarrassingly parallel problems*, repetitive tasks that have little or not
connection between each other more than the origin of the inputs and can be easily separated into
parallel tasks.  
These tasks are usually the ones we think about when we talk about `for` loops. One example can
be bootstrapping model coefficients (@lst-loop). For example, we are interested in the relationship
between sepal length and *Iris* species (example extracted from the `doParallel` package
vignette):

```{r}
#| lst-label: lst-loop
#| lst-cap: "Boostrapping model coefficients in iris dataset with a for loop"
#| warning: false

# libraries
library(dplyr)

# data needed
n_repetitions <- 1e4
res_coefs <- list()
iris_data <- iris |>
  dplyr::filter(Species != "setosa")
# we measure the time for illustration purposes
system.time({
  for (index in 1:n_repetitions) {
    sample_individuals <- sample(85, 85, replace = TRUE)
    model_res <- glm(
      iris_data[sample_individuals, "Species"] ~ iris_data[sample_individuals, "Petal.Length"],
      family = binomial
    )
    res_coefs[[index]] <- coefficients(model_res)
  }
})
```

We can see the user time (CPU time) is roughly the same as the elapsed time (real time), as we
should expect from a sequential `for` loop.

### lapply

The same problem can be solved with `lapply`, but we need to encapsulate the logic of the `for`
loop in a function (@lst-lapply):

```{r}
#| lst-label: lst-lapply
#| lst-cap: "Boostrapping model coefficients in iris dataset with lapply"
#| warning: false

# create the function to process data from one state
coef_function <- function(repetition) {
  sample_individuals <- sample(85, 85, replace = TRUE)
  model_res <- glm(
    iris_data[sample_individuals, "Species"] ~ iris_data[sample_individuals, "Petal.Length"],
    family = binomial
  )
  return(coefficients(model_res))
}
# number of repetitions
n_repetitions <- 1e4
# data
iris_data <- iris |>
  dplyr::filter(Species != "setosa")

# and now the lapply (we monitorize the time again for illustration purposes)
system.time(
  res_coefs <- lapply(1:n_repetitions, coef_function)
)
```

As we see, the time is the same as with the `for` loop, something we would expect.

### map

If using [`tidyverse` packages](https://www.tidyverse.org/), instead of `lapply` we will use `map`
function in the `purrr` package (@lst-purrr):

```{r}
#| lst-label: lst-purrr
#| lst-cap: "Boostrapping model coefficients in iris dataset with map"
#| warning: false

# libraries
library(purrr)

coef_function <- function(repetition) {
  sample_individuals <- sample(85, 85, replace = TRUE)
  model_res <- glm(
    iris_data[sample_individuals, "Species"] ~ iris_data[sample_individuals, "Petal.Length"],
    family = binomial
  )
  return(coefficients(model_res))
}
# number of repetitions
n_repetitions <- 1e4
# data
iris_data <- iris |>
  dplyr::filter(Species != "setosa")

# and now the map (we monitorize the time again for illustration purposes)
system.time({
  res_coefs <- purrr::map(1:n_repetitions, .f = coef_function)
})
```

Again times are similar to the other workflows.

## Ways to use parallelization in your code (`parallel`, `furrr`...)

If we can use loops, `lapply` or `map`, then we can parallelize without any problem. In this
section we will see the different options we can do it.

### Preparations

Before we start, we need to know how many cores are available in our system. This can be done
with `parallel::detectCores()`. In the system this document has been created the available cores
are:

```{r}
#| lst-label: lst-detectCores
#| lst-cap: "Numer of cores available"

library(parallel)
parallel::detectCores()
```

::: {.callout-tip}
In the following examples we will be using 4 cores, but if your system has less than that, please
change it to a valid number.
:::

### `foreach` and `doParallel`

In a very similar way to a `for` loop we can use the `foreach` and `doParallel` packages to build
a loop that will run the files in paralell (@lst-foreach):

```{r}
#| lst-label: lst-foreach
#| lst-cap: "Boostrapping model coefficients in iris dataset in parallel with a foreach"
#| warning: false
#| collapse: true

# libraries
library(parallel)
library(foreach)
library(doParallel)

# data needed
n_repetitions <- 1e4
res_coefs <- list()
iris_data <- iris |>
  dplyr::filter(Species != "setosa")

# set the number of cores to use, in this example 4
doParallel::registerDoParallel(cores = 4)

# foreach loop (for illustration purposes, we check the time used)
system.time(
  {res_coefs <- foreach::foreach(index = 1:n_repetitions) %dopar% {
    sample_individuals <- sample(85, 85, replace = TRUE)
    model_res <- glm(
      iris_data[sample_individuals, "Species"] ~ iris_data[sample_individuals, "Petal.Length"],
      family = binomial
    )
    coefficients(model_res)
  }}
)
```

As we can see, time has reduced almost four times when compared with processing the files
sequentially. We are really close to the ideal $1/4$ reduction in time we should expect from using
4 cores, but not quite, as starting the extra R processes, sending the data and retrieving the
results takes some time. With bigger datasets we can see that elapsed time increases because of this communication overload.

```{r}
#| include: false

# Stop the cluster
stopImplicitCluster()
```

### `mclapply`

If we prefer the `lapply` syntax, we can use `mclapply` to run the same expression concurrently.
`mclapply` belongs to the `parallel` pacakge and works exactly the same as lapply (@lst-mclapply):

```{r}
#| lst-label: lst-mclapply
#| lst-cap: "Boostrapping model coefficients in iris dataset in parallel with a mclapply"
#| warning: false

# create the function to process data from one state
coef_function <- function(repetition) {
  sample_individuals <- sample(85, 85, replace = TRUE)
  model_res <- glm(
    iris_data[sample_individuals, "Species"] ~ iris_data[sample_individuals, "Petal.Length"],
    family = binomial
  )
  return(coefficients(model_res))
}
# number of repetitions
n_repetitions <- 1e4
# data
iris_data <- iris |>
  dplyr::filter(Species != "setosa")

# and now the lapply (we monitorize the time again for illustration purposes)
system.time({
  res_coefs <- mclapply(1:n_repetitions, coef_function, mc.cores = 4)
})
```

We see again the time reduction in time with `mclapply`.

### `future_map`

`furrr` package offers parallelized versions of `purrr::map` family of functions. We can use it to
run the `map` example above in parallel (@lst-furrr):

```{r}
#| lst-label: lst-furrr
#| lst-cap: "Boostrapping model coefficients in iris dataset in parallel with a furrr"
#| warning: false

# libraries
library(future)
library(furrr)

coef_function <- function(repetition) {
  sample_individuals <- sample(85, 85, replace = TRUE)
  model_res <- glm(
    iris_data[sample_individuals, "Species"] ~ iris_data[sample_individuals, "Petal.Length"],
    family = binomial
  )
  return(coefficients(model_res))
}
# number of repetitions
n_repetitions <- 1e4
# data
iris_data <- iris |>
  dplyr::filter(Species != "setosa")

# setting core options
future::plan(future.callr::callr, workers = 4)

# and now the map (we monitorize the time again for illustration purposes)
system.time({
  state_models <- furrr::future_map(1:n_repetitions, .f = coef_function)
})
```

This is the method that returns the worst time running in parallel (but better than sequential).
This is because `future_map` works setting a more complete environment in the parallelized
processes that takes more time. In larger datasets and more complex functions, `future_map` is a
good option for paralellization.

::: {.callout-tip}
Calculating bootstrapped coefficients is a toy example for illustration purposes. There are R
packages that can bootstrap in more efficient ways, including parallelization already included,
like the [`boot` package](https://cran.r-project.org/package=boot).
:::

## How to check if parallelization is worthy the hassle {#hassle}

We have been using `system.time` to check the time our code takes to run. This is one way of
check if the benefit of parallelization outweighs the inconvenience of setting it up. Other ways
include using other benchmarking libraries, like `bench` or `rbenchmark`. Their documentation
explain everything you need to know about timing code.

In any case, as a rule of thumb, parallelization is not worthy in the following scenarios:

  1. We are parallelizing *fast* operations: In this scenario, when the operations we want to
     paralellize are fast (math operations, indexing or assigning) the overhead of sending data
     to cores, and retrieving and joining the results usually is greater than the time of performing
     the operation.

  1. We are parallelizing processes that involve sending a lot of data to each spawned parallel
     process. Think for example in working with a global raster at 5km resolution. If we want to
     parallelize a process involving calculation with this kind of big objects then the overhead of
     sending the data could be bigger than the process itself.
  
  1. Related to the previous, when performing memory intensive processes. Take into account that
     each spawned parallel process is going to need to use memory as in the sequential process.
     Parallelizing in the same computing multiplies the memory needed for almost the number of
     cores used, so is easy to run out of memory if we are not careful.
